{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zd717fK4b6Cp",
        "outputId": "4f929a39-b4c3-4e4a-a2e3-bdc4d00b62a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LangChain imports\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain_core.callbacks.base import BaseCallbackHandler\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "from operator import itemgetter\n",
        "\n",
        "st.set_page_config(page_title=\"File QA Chatbot\", page_icon=\"*\")\n",
        "st.title(\"Welcome to File QA RAG Chatbot\")\n",
        "\n",
        "@st.cache_resource(ttl=\"1h\")\n",
        "def configure_retriever(uploaded_files):\n",
        "    # Load the uploaded files into a LangChain Chroma vector store\n",
        "    docs = []\n",
        "    temp_dir = tempfile.TemporaryDirectory()\n",
        "    for file in uploaded_files:\n",
        "        temp_filepath = os.path.join(temp_dir.name, file.name)\n",
        "        with open(temp_filepath, \"wb\") as f:\n",
        "            f.write(file.getvalue())\n",
        "        loader = PyMuPDFLoader(temp_filepath)\n",
        "        docs.extend(loader.load())\n",
        "\n",
        "    if not docs:  # Add a check if any documents were loaded\n",
        "        return None\n",
        "\n",
        "    # Corrected the class name from RecursiveCharacterSplitter to RecursiveCharacterTextSplitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
        "    doc_chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "    if not doc_chunks: # Add a check if splitting created any chunks\n",
        "         return None\n",
        "\n",
        "    embeddings_model = OpenAIEmbeddings()\n",
        "    vectordb = Chroma.from_documents(doc_chunks, embeddings_model)\n",
        "\n",
        "    retriever = vectordb.as_retriever()\n",
        "    return retriever\n",
        "\n",
        "class StreamHandler(BaseCallbackHandler):\n",
        "    def __init__(self, container, initial_text=\"\"):\n",
        "        self.container = container\n",
        "        self.text = initial_text\n",
        "\n",
        "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
        "        self.text += token\n",
        "        self.container.markdown(self.text)\n",
        "\n",
        "# Initialize streamlit_msg_history unconditionally\n",
        "streamlit_msg_history = StreamlitChatMessageHistory(key=\"langchain_messages\")\n",
        "\n",
        "\n",
        "uploaded_files = st.sidebar.file_uploader(\n",
        "    label=\"Upload PDF files\", type=[\"pdf\"],\n",
        "    accept_multiple_files=True\n",
        ")\n",
        "\n",
        "if not uploaded_files:\n",
        "    st.info(\"Please upload one or more PDF files.\")\n",
        "    # It's generally better not to use st.stop() in interactive notebooks as it can\n",
        "    # prevent subsequent cells from running. Handle the None case for retriever instead.\n",
        "    # st.stop()\n",
        "    retriever = None # Explicitly set retriever to None if no files\n",
        "\n",
        "else:\n",
        "    retriever = configure_retriever(uploaded_files)\n",
        "\n",
        "# Only proceed to define the chain and chat if a retriever is successfully configured\n",
        "if retriever is not None:\n",
        "    chatgpt = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.1,\n",
        "                        streaming=True)\n",
        "\n",
        "    qa_template = \"\"\"\n",
        "        Use only the following pieces of context to answer the question at the end.\n",
        "        If you don't know the answer, just say that you don't know,\n",
        "        don't try to make up an answer. Keep the answer as concise as possible.\n",
        "\n",
        "        {context}\n",
        "\n",
        "        Question: {question}\n",
        "        \"\"\"\n",
        "\n",
        "    qa_prompt = ChatPromptTemplate.from_template(qa_template)\n",
        "\n",
        "    def format_docs(docs):\n",
        "        return \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "    # Define the chain only if retriever is not None\n",
        "    qa_rag_chain = (\n",
        "        {\n",
        "            \"context\": itemgetter(\"question\") | retriever | format_docs,\n",
        "            \"question\": itemgetter(\"question\")\n",
        "        }\n",
        "        | qa_prompt\n",
        "        | chatgpt\n",
        "    )\n",
        "\n",
        "    # This part of the code that handles chat input and response would also\n",
        "    # need to be inside this `if retriever is not None:` block.\n",
        "    # For brevity, I'm not including the full chat input/output loop here,\n",
        "    # as the original code didn't show it, but it's crucial for a functional app.\n",
        "    # You would likely have something like:\n",
        "    # user_input = st.chat_input(\"Your question:\")\n",
        "    # if user_input:\n",
        "    #     with st.chat_message(\"user\"):\n",
        "    #         st.markdown(user_input)\n",
        "    #     with st.chat_message(\"assistant\"):\n",
        "    #         stream_handler = StreamHandler(st.empty())\n",
        "    #         qa_rag_chain.invoke({\"question\": user_input}, {\"callbacks\": [stream_handler]})\n",
        "\n",
        "else:\n",
        "    # Display a message if the retriever could not be configured (e.g., no files uploaded)\n",
        "    st.info(\"Please upload PDF files to start the chat.\")\n",
        "    # Add an initial message to the history when no files are uploaded\n",
        "    if len(streamlit_msg_history.messages) == 0:\n",
        "        streamlit_msg_history.add_ai_message(\"Please upload PDF files to start the chat.\")\n",
        "\n",
        "\n",
        "# Display message history regardless, but the chat input will only appear\n",
        "# if retriever is not None in a real Streamlit app.\n",
        "for msg in streamlit_msg_history.messages:\n",
        "    st.chat_message(msg.type).write(msg.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rq3J3nXafggM",
        "outputId": "fce18eef-5b9b-4481-dfeb-84311760bb5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-22 05:36:19.557 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-22 05:36:19.562 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-22 05:36:19.563 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-22 05:36:19.567 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-22 05:36:19.568 Session state does not function when running a script without `streamlit run`\n",
            "2025-05-22 05:36:19.569 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-22 05:36:19.570 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-22 05:36:19.571 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-22 05:36:19.572 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-22 05:36:19.573 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-22 05:36:19.574 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-22 05:36:19.575 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-22 05:36:19.577 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-22 05:36:19.578 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-22 05:36:19.579 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-22 05:36:19.580 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-22 05:36:19.581 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-22 05:36:19.582 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-22 05:36:19.584 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-22 05:36:19.589 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-22 05:36:19.590 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1HULx3Dbpgd",
        "outputId": "615d94d7-cfa4-4681-bb00-4f41fbf0dc6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-22 05:58:13.604 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-22 05:58:13.605 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-22 05:58:13.607 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-22 05:58:13.608 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-22 05:58:13.609 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ],
      "source": [
        "class PostMessageHandler(BaseCallbackHandler):\n",
        "    def __init__(self, msg: st.write):\n",
        "      BaseCallbackHandler.__init__(self)\n",
        "      self.msg = msg\n",
        "      self.sources = []\n",
        "\n",
        "    def on_retriever_end(self, documents, *, run_id, present_run_id, **kwargs):\n",
        "      sources_ids = []\n",
        "      for d in documents:\n",
        "        metadata = {\n",
        "            \"sources\": d.metadata[\"source\"],\n",
        "            \"page\": d.metadata[\"page\"],\n",
        "            \"content\": d.page_content[:200]\n",
        "        }\n",
        "        idx = (metadata[\"source\"], metadata[\"page\"])\n",
        "        if idx not in sources_ids:\n",
        "          sources_ids.append(idx)\n",
        "          self.sources.append(metadata)\n",
        "\n",
        "    def on_llm_end(self, response, *, run_id, present_run_id, **kwargs):\n",
        "      if len(self.sources):\n",
        "        st.markdown(\"__Sources:__ \"+\"\\n\")\n",
        "        st.dataframe(data=pd. DataFrame(self.sources[:3]),\n",
        "                      width=1000)\n",
        "\n",
        "\n",
        "if user_prompt := st.chat_input():\n",
        "    st.chat_message(\"human\").write(user_prompt)\n",
        "\n",
        "    with st.chat_message(\"ai\"):\n",
        "      stream_handler = StreamHandler(st.empty())\n",
        "      sources_container = st.write(\"\")\n",
        "      pm_handler = PostMessageHandler(sources_container)\n",
        "      config = {\"callbacks\": [stream_handler, pm_handler]}\n",
        "      response = qa_rag_chain.invoke({\"question\": user_prompt}, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "JyIB1Vy0bpge"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py --server.port=8989 &>./logs.txt &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dm-phZvHbpgf",
        "outputId": "b32d8dce-b9dd-40d3-d7ec-0eca45a76fb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\n",
            "Streamlit App https://28e0-34-16-210-140.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "# Ensure you have the necessary library installed\n",
        "!pip install pyngrok pyyaml\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import yaml\n",
        "\n",
        "ngrok.kill()\n",
        "\n",
        "# Make sure 'ngrok_credentials.yml' exists in the same directory as this notebook\n",
        "# and contains your ngrok auth token in the format:\n",
        "# ngrok_key: your_ngrok_auth_token\n",
        "try:\n",
        "    with open(\"/content/drive/MyDrive/ngrok_credentials.yml\", \"r\") as file:\n",
        "        NGROK_AUTH_TOKEN = yaml.safe_load(file)\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN['ngrok_key'])\n",
        "\n",
        "    # Connect to the port where your Streamlit app is running (8989)\n",
        "    ngrok_tunnel = ngrok.connect(8989)\n",
        "    print(\"Streamlit App\", ngrok_tunnel.public_url)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: ngrok_credentials.yml not found. Please create this file and add your ngrok auth token.\")\n",
        "    print(\"The file should contain: ngrok_key: your_ngrok_auth_token\")\n",
        "except KeyError:\n",
        "    print(\"Error: ngrok_credentials.yml is missing the 'ngrok_key'. Please ensure the file is in the correct format.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "XeCRSCKDbpgg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}